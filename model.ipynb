{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> House Price Prediction Challenge</h1>\n",
    "<h5> By Franke van der Vorm and Bart van Moorsel</h5>\n",
    "\n",
    "<p> Project House Price Prediction Challenge (HPPC) is an 4-day assignment given by Avans Hogeschool in the Data Science for the Smart Industry minor on 31/10/2022. The original challenge can be found on <a href=\"https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques\">Kaggle</a>. The purpose of this challenge is to test the students ability to successfully make predictions based on historical data and evaluate the results. The given case of this challenge is to prediction house prices based on an unprepared dataset of around 80 collumns and 1460 rows. This dataset can be found <a href=\"https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data\">here</a>. </p>\n",
    "\n",
    "<p> This markdown is intended for demonstrating the solution made by the HPPC-team. In this IPython notebook, every collection of cells will be described in a way that explains what has been done and why it has been done. The solution is structured in 4 sections based on the best practices of the Cross Industry Standard Process for Data Mining (CRISP-DM).\n",
    "\n",
    "*   Importing the right modules\n",
    "*   Preprocessing the data\n",
    "*   Training the linear regression model\n",
    "*   Evaluating the model</p>\n",
    "\n",
    "<p> The chosen model for this assignment is a linear regression model. This model has been chosen because the target value is of numerical type, namely the price at which a house will be sold. In combination with an X amount of feature variables makes this a good option for linear regression.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Importing the right modules </h3>\n",
    "<p> Pandas and Numpy are used for efficient data manipulation. The machine learning module will be SKLearn, this will be used for fitting and evaluating the model. Lastly, matplotlib will be utilized for visualising the results. Next, the dataset will be loaded in and inspected. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"train.csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Preprocessing the data </h3>\n",
    "<p> As mentioned before, the dataset is unprepared and not ready for fitting. This means that data will have to be preprocessed first. This requires a significant understanding of the data. The data-understanding of the HCCP team is documented in <a href=\"\">the data-understanding report</a>, which also describes how certain data is filtered. </p>\n",
    "\n",
    "<p> The raw data could be categorized in two categories; numerical data and categorical data. Both need a different be approach. If there are no missing values for a categoral collumn, then it is ready for one-hot-encoding. \n",
    "In this coding section, a for loop is defined which iterates over all the categoral collumns specified in the ohe_list. This loop will select those collumns and apply one-hot-encoding to them. The result will be joined with the final X data. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter feature data\n",
    "X_final = pd.DataFrame()\n",
    "\n",
    "df = df.drop('Fence', axis=1)  \n",
    "df = df.drop('Alley', axis=1)  \n",
    "df = df.drop('PoolQC', axis=1)\n",
    "df = df.drop('MiscFeature', axis=1)\n",
    "df = df.drop('FireplaceQu', axis=1)\n",
    "\n",
    "df[\"LotFrontage\"] = df[\"LotFrontage\"].fillna(df[\"LotFrontage\"].mean())\n",
    "df[\"MasVnrArea\"] = df[\"MasVnrArea\"].fillna(df[\"MasVnrArea\"].mean())\n",
    "df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].fillna(2001)\n",
    "\n",
    "df['BsmtExposure'] = df['BsmtExposure'].fillna(df['BsmtExposure'].mode()[0])\n",
    "df['BsmtQual'] = df['BsmtQual'].fillna(df['BsmtQual'].mode()[0])\n",
    "df['BsmtCond'] = df['BsmtCond'].fillna(df['BsmtCond'].mode()[0])\n",
    "\n",
    "df['GarageCond'] = df['GarageCond'].fillna(df['GarageCond'].mode()[0])\n",
    "df['GarageQual'] = df['GarageQual'].fillna(df['GarageQual'].mode()[0])\n",
    "df['GarageType'] = df['GarageType'].fillna(df['GarageType'].mode()[0])\n",
    "df['GarageFinish'] = df['GarageFinish'].fillna(df['GarageFinish'].mode()[0])\n",
    "\n",
    "df['BsmtFinType1'] = df['BsmtFinType1'].fillna(df['BsmtFinType1'].mode()[0])\n",
    "df['BsmtFinType2'] = df['BsmtFinType2'].fillna(df['BsmtFinType2'].mode()[0])\n",
    "\n",
    "df['Electrical'] = df['Electrical'].fillna(df['Electrical'].mode()[0])\n",
    "df['MasVnrType'] = df['MasVnrType'].fillna(df['MasVnrType'].mode()[0])\n",
    "\n",
    "drop_index = df[(df[\"SalePrice\"] < 200000) & (df[\"OverallQual\"] > 8) & (df[\"GrLivArea\"] > 4000)].index\n",
    "df = df.drop(drop_index, axis = 0)\n",
    "\n",
    "y = df[\"SalePrice\"].values\n",
    "# df = df[[\"Neighborhood\"]]\n",
    "df = df.drop('SalePrice', axis=1)\n",
    "# df = df.drop('Id', axis=1)\n",
    "print(df.columns)\n",
    "df[\"MSSubClass\"] = list(map(lambda sub_class: \"MS_\" + str(sub_class) , df[\"MSSubClass\"])) # Remap sub_class to strings for one hot encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Do one hot encoding for every string column\n",
    "for column in df.columns:\n",
    "    selected_column = df[[column]]\n",
    "    c_type = type(selected_column.iloc[0, 0])\n",
    "    print(str(column) + \" of type \" + str(c_type))\n",
    "    if c_type == str or column == \"MSSubClass\":\n",
    "        # print(\"Detected string column: \" + column)\n",
    "        ohe_df = pd.get_dummies(selected_column, prefix=column, drop_first=True)\n",
    "        X_final = ohe_df.join(X_final)\n",
    "    elif c_type == np.int16 or c_type == np.int32 or c_type == np.int64 or c_type == np.float16 or c_type == np.float32 or c_type == np.float64:\n",
    "        X_final = selected_column.join(X_final)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "        \n",
    "    print(X_final)\n",
    "    # print(ohe_df)\n",
    "\n",
    "X = X_final.values\n",
    "# X_final.to_csv(\"temp.csv\")\n",
    "print(X.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Training the linear regression model </h3>\n",
    "\n",
    "<p> Now that the data is clean, it can be used for training the model. Training and test data will be split by the train_test_split method from sklearn. The training ratio will be 80% 20%. After splitting, a Linear Regression model will be trained on the training data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split train and test data\n",
    "print(\"X: \" + str(X.shape) + \" y: \" + str(y.shape))\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(type(X_train))\n",
    "\n",
    "# X_train = X_train.reshape(-1, 2)\n",
    "# print(X_train.shape)\n",
    "\n",
    "# X_test = X_test.reshape(-1, 1)\n",
    "\n",
    "# \"x_train size: \" + str(X_train.size) + \" y_train size: \" + str(y_train.size) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Evaluating the model </h3>\n",
    "\n",
    "<p> In order to determine if the model is valid, it will have to be evaluated. This will be done by calculating the mean squared error and by a 10-fold cross validation. Additionally, a root squared error is calculated which had been used in the early development stages.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate results\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "cv_scores = cross_val_score(reg, X, y, cv=kf)\n",
    "cv_mean = np.mean(cv_scores)\n",
    "\n",
    "# print(\"Mean squared error: \" + str(rmse))\n",
    "# print(\"Root squared error: \" + str(r_squared))\n",
    "print(\"Scores 10-fold CV: \" + str(cv_scores))\n",
    "print(\"Mean 10-fold CV: \" + str(cv_mean))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> To interpret if a model has been getting worse or better, it has to be compared with its history. This coding section makes a history dataframe containing the attempt number and the previously mentioned mean_cv and mean squared error. If there already exists such dataframe locally, it will be loaded in, if not, it will be made. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before evaluating on history, retrieve the history\n",
    "\n",
    "history = pd.DataFrame(columns=[\"Attempt\", \"mean_cv\", \"rmse\"] );\n",
    "try:\n",
    "    history = pd.read_csv(\"history.csv\")\n",
    "    print(\"Found history.csv with rows amount: \" + str(history.shape[0]))\n",
    "except FileNotFoundError:\n",
    "    print(\"history.csv not found, creating new one\")\n",
    "    history.to_csv('history.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> After the results get saved to history.csv, the history and the results will be displayed in a plot made with matplotlib. The plot shows an indication of progess and cross validation score, but should not be taken too literally as the model has been ran multiple times without changes or as the cross validation score has been calculated differentally at certain attempts. The purpose is to see how certain changes affect the performance of the model. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate based on history and visualize progress\n",
    "\n",
    "current_attempt = int(len(history.index))\n",
    "history.loc[len(history.index)] = [current_attempt, cv_mean, int(-1)]\n",
    "print(history[-5:-1])\n",
    "print(\"Current attempt: \\n\", history[-1:])\n",
    "history.to_csv('history.csv', index = False)\n",
    "\n",
    "plt.plot(history[\"Attempt\"], history[\"mean_cv\"])\n",
    "\n",
    "plt.xlabel(\"Attempts (current: {:.0f}\".format(current_attempt) + \")\")\n",
    "plt.xlim(0, np.size(history[\"Attempt\"].values))\n",
    "# plt.ylim(min_r_squared - visual_margin, max_r_squared + visual_margin)\n",
    "plt.ylabel(\"Mean cross_val_score (current: {:.2f}\".format(cv_mean) + \")\")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"10f Cross-Validation score progress\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12fdc936a54ce685e4c9b2f8277f7451ffeec2cb8757075b37386f7baad81d01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
