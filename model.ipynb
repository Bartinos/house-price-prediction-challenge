{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> House Price Prediction Challenge</h1>\n",
    "<h5> By Franke van der Vorm and Bart van Moorsel</h5>\n",
    "\n",
    "<p> Project House Price Prediction Challenge (HPPC) is an 4-day assignment given by Avans Hogeschool in the Data Science for the Smart Industry minor on 31/10/2022. The original challenge can be found on <a href=\"https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques\">Kaggle</a>. The purpose of this challenge is to test the students ability to successfully make predictions based on historical data and evaluate the results. The given case of this challenge is to prediction house prices based on an unprepared dataset of around 80 collumns and 1460 rows. This dataset can be found <a href=\"https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data\">here</a>. </p>\n",
    "\n",
    "<p> This markdown is intended for demonstrating the solution made by the HPPC-team. In this IPython notebook, every collection of cells will be described in a way that explains what has been done and why it has been done. The solution is structured in 4 sections based on the best practices of the Cross Industry Standard Process for Data Mining (CRISP-DM).\n",
    "\n",
    "*   Importing the right modules\n",
    "*   Preprocessing the data\n",
    "*   Training the linear regression model\n",
    "*   Evaluating the model</p>\n",
    "\n",
    "<p> The chosen model for this assignment is a linear regression model. This model has been chosen because the target value is of numerical type, namely the price at which a house will be sold. In combination with an X amount of feature variables makes this a good option for linear regression.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Importing the right modules </h3>\n",
    "<p> Pandas and Numpy are used for efficient data manipulation. The machine learning module will be SKLearn, this will be used for fitting and evaluating the model. Lastly, matplotlib will be utilized for visualising the results. Next, the dataset will be loaded in and inspected. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"train.csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Preprocessing the data </h3>\n",
    "<p> As mentioned before, the dataset is unprepared and not ready for fitting. This means that data will have to be preprocessed first. This requires a significant understanding of the data. The data-understanding of the HCCP team is documented in <a href=\"\">the data-understanding report</a>, which also describes how certain data is filtered. </p>\n",
    "\n",
    "<p> The raw data could be categorized in two categories; numerical data and categorical data. Both need a different be approach. If there are no missing values for a categoral collumn, then it is ready for one-hot-encoding. In this coding section, a for loop is defined which iterates over all the categoral collumns specified in the ohe_list.   </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter feature data\n",
    "X_unproc = df[[\"LotArea\",\"YrSold\"]]\n",
    "ohe_list = [\"Neighborhood\", \"ExterQual\"]\n",
    "X_final = pd.DataFrame()\n",
    "\n",
    "#Do one hot encoding for every collumn specified in ohe_list\n",
    "for collumn in ohe_list:\n",
    "    pre_ohe_df = df[[collumn]]\n",
    "    ohe_df = pd.get_dummies(pre_ohe_df, prefix=collumn)\n",
    "    X_final = ohe_df.join(X_final)\n",
    "    # print(X_final)\n",
    "    # print(ohe_df)\n",
    "\n",
    "# print(X)\n",
    "# X[\"YrSold\"] = list(map(lambda year: year - 2006, df[\"YrSold\"])) #example of remapping column values \n",
    "# print(list(X[\"YrSold\"]))\n",
    "X_final = X_unproc.join(X_final)\n",
    "# X_final.to_csv(\"temp.csv\")\n",
    "X = X_final.values\n",
    "\n",
    "\n",
    "y = df[\"SalePrice\"].values\n",
    "print(X.shape)\n",
    "# print(y.shape)\n",
    "# # X = X.reshape(1460, 54)\n",
    "# print(X.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "# X_train = X_train.reshape(-1, 2)\n",
    "# print(X_train.shape)\n",
    "\n",
    "# X_test = X_test.reshape(-1, 1)\n",
    "\n",
    "# \"x_train size: \" + str(X_train.size) + \" y_train size: \" + str(y_train.size) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train LinearRegression model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate results\n",
    "# X_test = X_test.reshape(-1, 2)\n",
    "print(X_test.shape)\n",
    "y_pred = reg.predict(X_test)\n",
    "r_squared = reg.score(X_test, y_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "kf = KFold(n_splits=6, shuffle=True)\n",
    "cv_scores = cross_val_score(reg, X, y, cv=kf)\n",
    "cv_mean = np.mean(cv_scores)\n",
    "\n",
    "print(\"Mean squared error: \" + str(rmse))\n",
    "print(\"Root squared error: \" + str(r_squared))\n",
    "print(\"Mean 6-fold CV: \" + str(cv_mean))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before evaluating on history, retrieve the history\n",
    "\n",
    "history = pd.DataFrame(columns=[\"Attempt\", \"mean_cv\", \"rmse\"] );\n",
    "try:\n",
    "    history = pd.read_csv(\"history.csv\")\n",
    "    print(\"Found history.csv with rows amount: \" + str(history.shape[0]))\n",
    "except FileNotFoundError:\n",
    "    print(\"history.csv not found, creating new one\")\n",
    "    history.to_csv('history.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate based on history and visualize progress\n",
    "\n",
    "\n",
    "history.loc[len(history.index)] = [int(len(history.index)), cv_mean, int(rmse)]\n",
    "print(history[-5:-1])\n",
    "print(\"Current attempt: \", history[-1:])\n",
    "history.to_csv('history.csv', index = False)\n",
    "\n",
    "plt.plot(history[\"Attempt\"], history[\"mean_cv\"])\n",
    "min_cv_mean = np.min(history[\"mean_cv\"].values)\n",
    "max_cv_mean = np.max(history[\"mean_cv\"].values)\n",
    "\n",
    "visual_margin = (max_cv_mean - min_cv_mean) * 0.5 + 0.001 #Makes the graph more eye friendly\n",
    "\n",
    "plt.xlabel(\"Attempts\")\n",
    "plt.xlim(0, np.size(history[\"Attempt\"].values))\n",
    "# plt.ylim(min_r_squared - visual_margin, max_r_squared + visual_margin)\n",
    "plt.ylabel(\"Mean cross_val_score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Cross-Validation score progress\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6dfd9a2f397d2409bfc19b43597bfa85f98f14251ece3d9ffa4569976ee80c50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
